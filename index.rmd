Practical Machine Learning - Prediction Assignment Writeup
========================================================

## Background 

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.




## Getting started

A key early component of any machine learning project involves managing and understanding the data.

## Loading data

The data come CSV (comma separated values) format and they can be easily loaded by using R functions:

```{r}
data <- read.csv("pml-training.csv",  na.strings = "NA", stringsAsFactors = FALSE)
unlabeled <- read.csv("pml-testing.csv",  na.strings = "NA",  stringsAsFactors = FALSE)
```

The training set consists of 19622 observations of 160 variables (features), witch data$classe is the dependent variable.

```{r}
dim(data)
```

Looking up the dataset we can see that many of the 159 features are empty in most of the observations:

```{r}
sum(complete.cases(data))
```


## Exploring data

I choose 52 features and discanding some features that only introduced noise and can cause overfitting such as X, user_name, raw_timestamp_part1, raw_timestamp_part2, cvtd_timestamp, new_window and num_window.


```{r}
columns <- c("roll_belt", "pitch_belt", "yaw_belt", "total_accel_belt", "gyros_belt_x", 
    "gyros_belt_y", "gyros_belt_z", "accel_belt_x", "accel_belt_y", "accel_belt_z", "magnet_belt_x",
    "magnet_belt_y", "magnet_belt_z", "roll_arm", "pitch_arm", "yaw_arm", "total_accel_arm",
    "gyros_arm_x", "gyros_arm_y", "gyros_arm_z", "accel_arm_x", "accel_arm_y", "accel_arm_z",
    "magnet_arm_x", "magnet_arm_y", "magnet_arm_z", "roll_dumbbell", "pitch_dumbbell", 
    "yaw_dumbbell", "total_accel_dumbbell", "gyros_dumbbell_x", "gyros_dumbbell_y", 
    "gyros_dumbbell_z", "accel_dumbbell_x", "accel_dumbbell_y", "accel_dumbbell_z", 
    "magnet_dumbbell_x", "magnet_dumbbell_y", "magnet_dumbbell_z", "roll_forearm", 
    "pitch_forearm", "yaw_forearm", "total_accel_forearm", "gyros_forearm_x", "gyros_forearm_y",
    "gyros_forearm_z", "accel_forearm_x", "accel_forearm_y", "accel_forearm_z", "magnet_forearm_x",
    "magnet_forearm_y", "magnet_forearm_z")

training <- data [, columns]

```

With this transformation, the resulting  dataser is now of 19622 observations of 53 features.

```{r}
dim(training)
sum(complete.cases(training))
```

Now, once cleaned the dataset try to analyze the correlation between features.

```{r}
correlation <- cor(training)
library(lattice)
palette <- colorRampPalette(c("blue", "yellow"), space = "rgb")
levelplot(correlation, 
          main="Correlation Level Plot", xlab="",ylab="",aspect=1,
          col.regions=palette(120), pretty=TRUE,
          cuts=100, at=seq(0,1,0.01),
          scales=list(x=list(rot=90)) )
```

Most of features have no too many degree of correlation. However, some other features that are correlated. I going to reduce the number of features, removing the correlated ones.

```{r}
library("caret")

highCorr <- findCorrelation( correlation, 0.90)

columns <- columns [-highCorr]

training <- data [, c(columns,"classe")]

dim(training)
```


## Getting a Predictive Model

I train a classifier by using the GBM tool (Stochastic Gradient Boosting). I do not use cross validation and run for 100 iterations with a step size of 0.1.


```{r}
set.seed(1)

library(caret)
library(gbm)

inTrain <- createDataPartition(training$classe, p = 0.6, list = FALSE)
myTraining <- training [inTrain,]
myTesting <- training[-inTrain,]




control <- trainControl(method = "none")
tune <- expand.grid(.interaction.depth = 4, .n.trees = 100, .shrinkage = 0.1)

model <- train ( classe ~ ., data=myTraining , method="gbm" , tuneGrid=tune, trControl = control, verbose=FALSE)

model
```


## Validating the model

Let's see how well performs the model against the testing dataset. Then, I calcule the confunsion matrix by using the prediction on the testing data. The confusion matrix indicate that the model fit well the training set. The accurancy is near 0.96.

```{r}
prediction <- predict (model, newdata = myTesting[, columns] )

confMatrix <- confusionMatrix ( prediction , myTesting$classe)

confMatrix
```


## Generating the answers

I use the obtained model to predict labels for the unlabeled dataset.

```{r}
answers <- predict (model, newdata = unlabeled[, columns] )

answers
```
